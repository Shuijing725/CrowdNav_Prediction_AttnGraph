import copy
import torch
import torch.nn.functional as F
from torch.autograd import Variable
from torch.nn.modules.container import ModuleList


def _get_activation_fn(activation):
    if activation == "relu":
        return F.relu
    elif activation == "gelu":
        return F.gelu
    raise RuntimeError("activation should be relu/gelu, not {}".format(activation))

def _get_clones(module, N):
    return ModuleList([copy.deepcopy(module) for i in range(N)])

def gumbel_softmax(logits, tau=1, hard=False, eps=1e-10):
    y_soft = gumbel_softmax_sample(logits, tau=tau, eps=eps)
    if hard:
        shape = logits.size()
        _, k = y_soft.data.max(-1)
        y_hard = torch.zeros(*shape)
        if y_soft.is_cuda:
            y_hard = y_hard.to("cuda")
        if y_soft.is_mps:
            y_hard = y_hard.to("mps")
        y_hard = y_hard.zero_().scatter_(-1, k.view(shape[:-1] + (1,)), 1.0)
        y = Variable(y_hard - y_soft.data) + y_soft
    else:
        y = y_soft
    return y

def gumbel_softmax_sample(logits, tau=1, eps=1e-10):
    gumbel_noise = sample_gumbel(logits.size(), eps=eps)
    if logits.is_cuda:
        gumbel_noise = gumbel_noise.to("cuda")
    if logits.is_mps:
        gumbel_noise = gumbel_noise.to("mps")
    y = logits + Variable(gumbel_noise)
    return F.softmax(y / tau, dim=-1)

def sample_gumbel(shape, eps=1e-10):
    uniform = torch.rand(shape).float()
    return - torch.log(eps - torch.log(uniform + eps))